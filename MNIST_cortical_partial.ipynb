{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPw6kICcfM49sTaqGYV8mgS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zmarquis13/cortical_model_MNIST/blob/main/MNIST_cortical_partial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jieD6EIVLH9H"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")"
      ],
      "metadata": {
        "id": "FJrRcI1cLUyQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "065f6d3a-6450-4159-ea01-00a2dbbe91a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 17809783.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 680052.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 5391524.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 1244075.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.1\n",
        "batch_size = 64\n",
        "epochs = 10"
      ],
      "metadata": {
        "id": "4zquxZNe6hbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "71AIRkdGR3AR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display image and label.\n",
        "train_features, train_labels = next(iter(train_dataloader))\n",
        "print(f\"Feature batch shape: {train_features.size()}\")\n",
        "print(f\"Labels batch shape: {train_labels.size()}\")\n",
        "img = train_features[0].squeeze()\n",
        "label = train_labels[0]\n",
        "plt.imshow(img, cmap=\"gray\")\n",
        "plt.show()\n",
        "print(f\"Label: {label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "q8tkOvQORp-J",
        "outputId": "ce2a4b3d-6770-4c98-b319-8c5c1bd549a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
            "Labels batch shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jvLxBZk7Or_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dev = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "metadata": {
        "id": "yx2b_8HNNSSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        #can't do sequential because I want parallel computation\n",
        "        #self.input = nn.Linear(28*28, 512)\n",
        "        self.input = nn.Linear(28*28, 512)\n",
        "        self.hidden = nn.Linear(512, 512)\n",
        "\n",
        "        #cortex hidden layers\n",
        "        self.cortex1 = nn.Linear(512, 10)\n",
        "        self.cortex2 = nn.Linear(512, 10)\n",
        "        self.cortex3 = nn.Linear(512, 10)\n",
        "        self.cortex4 = nn.Linear(512, 10)\n",
        "        self.cortex5 = nn.Linear(512, 10)\n",
        "        #self.cortex6 = nn.Linear(512, 10)\n",
        "        #self.cortex7 = nn.Linear(512, 10)\n",
        "        #self.cortex8 = nn.Linear(512, 10)\n",
        "        #self.cortex9 = nn.Linear(512, 10)\n",
        "        #self.cortex10 = nn.Linear(512, 10)\n",
        "\n",
        "        #outputs\n",
        "        self.output1 = nn.Linear(10, 1)\n",
        "        self.output2 = nn.Linear(10, 1)\n",
        "        self.output3 = nn.Linear(10, 1)\n",
        "        self.output4 = nn.Linear(10, 1)\n",
        "        self.output5 = nn.Linear(10, 1)\n",
        "        self.output6 = nn.Linear(10, 1)\n",
        "        #self.output7 = nn.Linear(10, 1)\n",
        "        #self.output8 = nn.Linear(10, 1)\n",
        "        #self.output9 = nn.Linear(10, 1)\n",
        "        #self.output10 = nn.Linear(10, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #print(x.shape)\n",
        "        x = self.input(x)\n",
        "\n",
        "        #print(x.shape)\n",
        "\n",
        "        x = self.hidden(x)\n",
        "\n",
        "        #print(x.shape)\n",
        "\n",
        "        #print(\"break\")\n",
        "\n",
        "        x1 = torch.relu(self.cortex1(x))\n",
        "        x2 = torch.relu(self.cortex2(x))\n",
        "        x3 = torch.relu(self.cortex3(x))\n",
        "        x4 = torch.relu(self.cortex4(x))\n",
        "        x5 = torch.relu(self.cortex5(x))\n",
        "        #x6 = torch.relu(self.cortex6(x))\n",
        "        #x7 = torch.relu(self.cortex7(x))\n",
        "        #x8 = torch.relu(self.cortex8(x))\n",
        "        #x9 = torch.relu(self.cortex9(x))\n",
        "        #x10 = torch.relu(self.cortex10(x))\n",
        "\n",
        "        #cortices have shape ([64, 10])\n",
        "        #print(x1.shape)\n",
        "\n",
        "        x1 = self.output1(x1)\n",
        "        x2 = self.output2(x2)\n",
        "        x3 = self.output3(x3)\n",
        "        x4 = self.output4(x4)\n",
        "        x5 = self.output5(x5)\n",
        "        #x6 = self.output6(x6)\n",
        "        #x7 = self.output7(x7)\n",
        "        #x8 = self.output8(x8)\n",
        "        #x9 = self.output9(x9)\n",
        "        #x10 = self.output10(x10)\n",
        "\n",
        "        #outputs have shape ([64, 1])\n",
        "        #print(x1)\n",
        "\n",
        "        #add x6-x10 to make complete\n",
        "        x = torch.cat((x1, x2, x3, x4, x5), 1)\n",
        "\n",
        "        #concatenating gives shape ([64, 5])\n",
        "        #print(x.shape)\n",
        "\n",
        "        #basically want each value to be 1 - the sum of the five predicted vales in x\n",
        "        temp = torch.ones(x.size(0), 1)\n",
        "        for i in range (x.size(0)):\n",
        "          sum = 0\n",
        "          for j in range(5):\n",
        "            sum += x[1][j]\n",
        "            temp[i] = 1 - torch.sigmoid(sum)\n",
        "\n",
        "        zeros = torch.zeros(x.size(0), 4)\n",
        "        intermediate = torch.cat((temp, zeros), 1)\n",
        "\n",
        "        x = torch.cat((x, intermediate), 1)\n",
        "\n",
        "        #print(x)\n",
        "\n",
        "        #x = torch.cat((x, ones), 1)\n",
        "\n",
        "        return x\n",
        "\n",
        "model = NeuralNetwork()"
      ],
      "metadata": {
        "id": "2vN0HUYENdif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#may need to define a more complicated loss function because the output is 1x5 and the ground truth is 1x10\n",
        "#if I can \"cut\" the ground truth to its first 5 values (or extend output to 1x10 with zeros), then I can get an actual loss function\n",
        "#extending the output seems easier because then I can use the default cross entropy loss\n",
        "\n",
        "#also something to consider: data will be 50% \"null\" and 10% of each of the numbers, may cause bias\n",
        "#towards null but this is solvable by only including 1/6 of the null values\n",
        "#or I could make 10 cortices and include 1/11th static\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n"
      ],
      "metadata": {
        "id": "YsmfJWL5NeYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    # Set the model to training mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "\n",
        "    #y is a 1x64 tensor\n",
        "\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "\n",
        "        #print(y)\n",
        "\n",
        "        #print(\"break\")\n",
        "\n",
        "        X = X.view(X.shape[0],784)\n",
        "        #print(len(X))\n",
        "\n",
        "\n",
        "        pred = model(X)\n",
        "\n",
        "        #program break\n",
        "        #if y > 6:\n",
        "        #  y = 6\n",
        "\n",
        "        for i in range(y.size(0)):\n",
        "              if y[i] > 5:\n",
        "                y[i] = 5\n",
        "\n",
        "        #pred = torch.cat(pred, ([0.5]))\n",
        "\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 99:\n",
        "            loss, current = loss.item(), batch * batch_size + len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    #size = 10000\n",
        "    num_batches = len(dataloader)\n",
        "    #num batches = 10000/64\n",
        "    #print (num_batches)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
        "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
        "\n",
        "    # Need some way to make a null prediction actually count\n",
        "    # Basically, want output to not be argmaxed (or argmaxed with some kind of default value added on)\n",
        "    # Ex. if output is 0.3, 0.1, 0.0. 0.2, 0.1, could add 0.5 on the end and make it represent the null before argmaxing\n",
        "    # Main issue: outputs may not be scaled 0 to 1\n",
        "    # Also, then how to train\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X = X.view(X.shape[0],784)\n",
        "\n",
        "            pred = model(X)\n",
        "\n",
        "            #how is this actually calculating whether or not it's correct?\n",
        "            #note: any changes here will also probably have to be made to train\n",
        "\n",
        "            #y is a 1x64 tensor of the 64 ground truths for the batch\n",
        "            #could iterate through y and make every value above 6 equal to 6\n",
        "\n",
        "            for i in range(y.size(0)):\n",
        "              if y[i] > 5:\n",
        "                y[i] = 5\n",
        "\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Number Test Results: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "DNGpctX9Xonc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_static(mode, loss_fn):\n",
        "  model.eval()\n",
        "  #size of about 10000\n",
        "\n",
        "  with torch.no_grad():\n",
        "    size = 157*64\n",
        "    num_batches = 157\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    for i in range (num_batches):\n",
        "      #figure out how to set all the labels to 6\n",
        "      static = torch.rand(64, 784)\n",
        "      pred = model(static)\n",
        "\n",
        "      #long to resolve runtime error\n",
        "      y = torch.ones(64, dtype=torch.long)\n",
        "      y = y * 5\n",
        "\n",
        "      test_loss += loss_fn(pred, y).item()\n",
        "      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "  test_loss /= num_batches\n",
        "  #fixed size to try to resolve error\n",
        "  correct /= 10048.0\n",
        "  print(f\"Static Test Results: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "CMqtJMmbbEAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "    test_static(model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98F0IYYJXpvX",
        "outputId": "91f2568b-9a39-40c1-a31f-8f4efc5bccb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.964890  [ 6400/60000]\n",
            "loss: 0.718468  [12800/60000]\n",
            "loss: 0.632759  [19200/60000]\n",
            "loss: 0.750583  [25600/60000]\n",
            "loss: 0.815116  [32000/60000]\n",
            "loss: 0.719433  [38400/60000]\n",
            "loss: 0.809382  [44800/60000]\n",
            "loss: 0.580018  [51200/60000]\n",
            "loss: 0.799470  [57600/60000]\n",
            "Number Test Results: \n",
            " Accuracy: 91.8%, Avg loss: 0.694228 \n",
            "\n",
            "Static Test Results: \n",
            " Accuracy: 99.7%, Avg loss: 0.932873 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.835070  [ 6400/60000]\n",
            "loss: 0.743100  [12800/60000]\n",
            "loss: 0.726761  [19200/60000]\n",
            "loss: 0.561136  [25600/60000]\n",
            "loss: 0.698230  [32000/60000]\n",
            "loss: 0.690678  [38400/60000]\n",
            "loss: 0.907619  [44800/60000]\n",
            "loss: 0.586817  [51200/60000]\n",
            "loss: 0.615545  [57600/60000]\n",
            "Number Test Results: \n",
            " Accuracy: 91.7%, Avg loss: 0.651008 \n",
            "\n",
            "Static Test Results: \n",
            " Accuracy: 98.8%, Avg loss: 0.947946 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.721680  [ 6400/60000]\n",
            "loss: 0.789012  [12800/60000]\n",
            "loss: 0.705962  [19200/60000]\n",
            "loss: 0.418907  [25600/60000]\n",
            "loss: 0.619775  [32000/60000]\n",
            "loss: 0.622033  [38400/60000]\n",
            "loss: 0.578414  [44800/60000]\n",
            "loss: 0.713986  [51200/60000]\n",
            "loss: 0.598063  [57600/60000]\n",
            "Number Test Results: \n",
            " Accuracy: 92.6%, Avg loss: 0.646055 \n",
            "\n",
            "Static Test Results: \n",
            " Accuracy: 75.6%, Avg loss: 1.191397 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.736833  [ 6400/60000]\n",
            "loss: 0.693134  [12800/60000]\n",
            "loss: 0.688770  [19200/60000]\n",
            "loss: 0.692930  [25600/60000]\n",
            "loss: 0.577116  [32000/60000]\n",
            "loss: 0.584467  [38400/60000]\n",
            "loss: 0.417202  [44800/60000]\n",
            "loss: 0.609826  [51200/60000]\n",
            "loss: 0.596057  [57600/60000]\n",
            "Number Test Results: \n",
            " Accuracy: 94.9%, Avg loss: 0.606946 \n",
            "\n",
            "Static Test Results: \n",
            " Accuracy: 99.3%, Avg loss: 0.999537 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.648572  [ 6400/60000]\n",
            "loss: 0.582300  [12800/60000]\n",
            "loss: 0.764017  [19200/60000]\n",
            "loss: 0.615747  [25600/60000]\n",
            "loss: 0.466297  [32000/60000]\n",
            "loss: 0.577867  [38400/60000]\n",
            "loss: 0.589587  [44800/60000]\n",
            "loss: 0.552180  [51200/60000]\n",
            "loss: 0.574477  [57600/60000]\n",
            "Number Test Results: \n",
            " Accuracy: 93.2%, Avg loss: 0.612852 \n",
            "\n",
            "Static Test Results: \n",
            " Accuracy: 99.5%, Avg loss: 0.966577 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.579282  [ 6400/60000]\n",
            "loss: 0.747162  [12800/60000]\n",
            "loss: 0.534964  [19200/60000]\n",
            "loss: 0.672705  [25600/60000]\n",
            "loss: 0.608395  [32000/60000]\n",
            "loss: 0.544557  [38400/60000]\n",
            "loss: 0.622863  [44800/60000]\n",
            "loss: 0.616027  [51200/60000]\n",
            "loss: 0.666368  [57600/60000]\n",
            "Number Test Results: \n",
            " Accuracy: 94.3%, Avg loss: 0.581008 \n",
            "\n",
            "Static Test Results: \n",
            " Accuracy: 99.9%, Avg loss: 0.929469 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.625791  [ 6400/60000]\n",
            "loss: 0.441675  [12800/60000]\n",
            "loss: 0.573689  [19200/60000]\n",
            "loss: 0.701211  [25600/60000]\n",
            "loss: 0.454039  [32000/60000]\n",
            "loss: 0.666832  [38400/60000]\n",
            "loss: 0.612799  [44800/60000]\n",
            "loss: 0.557129  [51200/60000]\n",
            "loss: 0.693919  [57600/60000]\n",
            "Number Test Results: \n",
            " Accuracy: 95.6%, Avg loss: 0.566714 \n",
            "\n",
            "Static Test Results: \n",
            " Accuracy: 99.4%, Avg loss: 0.942743 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.559609  [ 6400/60000]\n",
            "loss: 0.703341  [12800/60000]\n",
            "loss: 0.740612  [19200/60000]\n",
            "loss: 0.561763  [25600/60000]\n",
            "loss: 0.534129  [32000/60000]\n",
            "loss: 0.508756  [38400/60000]\n",
            "loss: 0.476890  [44800/60000]\n",
            "loss: 0.559999  [51200/60000]\n",
            "loss: 0.572304  [57600/60000]\n",
            "Number Test Results: \n",
            " Accuracy: 95.6%, Avg loss: 0.558161 \n",
            "\n",
            "Static Test Results: \n",
            " Accuracy: 64.8%, Avg loss: 1.209788 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.635294  [ 6400/60000]\n",
            "loss: 0.629752  [12800/60000]\n",
            "loss: 0.571528  [19200/60000]\n",
            "loss: 0.425644  [25600/60000]\n",
            "loss: 0.518351  [32000/60000]\n",
            "loss: 0.412812  [38400/60000]\n",
            "loss: 0.556621  [44800/60000]\n",
            "loss: 0.578762  [51200/60000]\n",
            "loss: 0.567526  [57600/60000]\n",
            "Number Test Results: \n",
            " Accuracy: 95.8%, Avg loss: 0.557719 \n",
            "\n",
            "Static Test Results: \n",
            " Accuracy: 77.0%, Avg loss: 1.146514 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.478937  [ 6400/60000]\n",
            "loss: 0.678218  [12800/60000]\n",
            "loss: 0.521346  [19200/60000]\n",
            "loss: 0.487436  [25600/60000]\n",
            "loss: 0.554781  [32000/60000]\n",
            "loss: 0.459144  [38400/60000]\n",
            "loss: 0.522787  [44800/60000]\n",
            "loss: 0.495624  [51200/60000]\n",
            "loss: 0.543996  [57600/60000]\n",
            "Number Test Results: \n",
            " Accuracy: 96.4%, Avg loss: 0.548488 \n",
            "\n",
            "Static Test Results: \n",
            " Accuracy: 68.3%, Avg loss: 1.231870 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    }
  ]
}